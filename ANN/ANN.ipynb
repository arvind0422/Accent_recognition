{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "import sklearn.metrics as skm\n",
    "import pandas as pd\n",
    "from keras.layers import Dense, Dropout, Input, Activation, Softmax\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.models import Model, load_model, save_model\n",
    "from keras.utils import plot_model\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing stuff in console for debugging purposes.\n",
    "import sys\n",
    "import os\n",
    "try:\n",
    "    os.system(\"rm log.txt\")\n",
    "    sys.stdout = open(\"log.txt\",'w')\n",
    "except:\n",
    "    sys.stdout = open(\"log.txt\", 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture is defined here.\n",
    "def get_model(feat_shape=200,num_accents=5):\n",
    "    \n",
    "    input1 = Input(shape=(feat_shape,),name=\"input\")\n",
    "    \n",
    "    dense = Dense(feat_shape)(input1)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Activation(\"relu\")(dense)\n",
    "    \n",
    "    dense = Dropout(0.5)(dense)\n",
    "    dense = Dense(feat_shape)(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Activation(\"relu\")(dense)\n",
    "    \n",
    "    dense = Dropout(0.25)(dense)\n",
    "    dense = Dense(int(feat_shape/2))(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Activation(\"relu\")(dense)\n",
    "    \n",
    "    dense = Dropout(0)(dense)\n",
    "    dense = Dense(int(feat_shape/4))(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Activation(\"relu\")(dense)\n",
    "    \n",
    "    dense = Dense(num_accents)(dense)\n",
    "    output = Softmax(axis=0,name=\"output\")(dense)\n",
    "    print(output.shape)\n",
    "    \n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_accents=5\n",
    "model = get_model(200,num_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model,\"model.png\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_epochs = 3\n",
    "\n",
    "old_metric = -1 # Depends. Either very low or very high initial value.\n",
    "patience = 1\n",
    "glob_count=0\n",
    "\n",
    "training_files = open(\"trn_fstat.list\").readlines()\n",
    "num_training_samples = len(training_files)\n",
    "print(\"Number of training samples: \"+str(num_training_samples))\n",
    "\n",
    "# Load Testing Data.\n",
    "testing_files = open(\"tst_fstat.list\").readlines()\n",
    "num_testing_samples = len(testing_files)\n",
    "print(\"Number of testing samples: \"+str(num_testing_samples))\n",
    "test_in = []\n",
    "test_labels = []\n",
    "for i in range(num_testing_samples):\n",
    "    f = testing_files[i].split() # Asssuming the same format as in seniors' ann.py which uses the list file.\n",
    "    f_in = open(f[0],\"r\")\n",
    "    test_in.append(np.fromfile(f_in, dtype=np.double))\n",
    "    test_labels.append(int(f[1]))\n",
    "test_in = np.asarray(test_in)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "while True: \n",
    "    \n",
    "    # List file creation\n",
    "    \"\"\"\n",
    "    Ideally, we want a list array which is shuffled and can be read in by the training script. So, based on the\n",
    "    batch_size, we create a new list file that has the shuffled data. That must be done here.\n",
    "\n",
    "    Maybe we can have an outer loop to control which particular label dominates the training data - equivalent to\n",
    "    fine tuning for each label. We can maybe do this if accuracy is low after basic training.\n",
    "\n",
    "    This makes changes to training_files\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch_no in range(num_epochs):\n",
    "        \n",
    "        # For now, it is just shuffling the input data.\n",
    "        for __shuff__ in range(3):\n",
    "            shuffle(training_files)\n",
    "        \n",
    "        glob_count += 1 # Keeping track of total number of epochs.\n",
    "        print(\"Epoch number: \"+str(glob_count))\n",
    "        \n",
    "        # The following for loop running once is equivalent to one epoch on the whole training data.\n",
    "        for i in range(int(num_training_samples/batch_size)):\n",
    "\n",
    "            # Get a mini-batch of file names from the training files.\n",
    "            try\n",
    "                this_batch_files = training_files[batch_size*i:(batch_size*i)+batch_size]\n",
    "            except:\n",
    "                this_batch_files = training_files\n",
    "\n",
    "            # Load data for the particular mini-batch.\n",
    "            mini_batch_in = []\n",
    "            mini_batch_labels = []\n",
    "            for i in range(batch_size):\n",
    "                f = this_batch_files[i].split() # Split into file name and label\n",
    "                f_in=open(f[0],\"r\")\n",
    "                mini_batch_in.append(np.fromfile(f_in, dtype=np.double))\n",
    "                mini_batch_labels.append(int(f[1]))\n",
    "            mini_batch_in = np.asarray(mini_batch_in)\n",
    "            mini_batch_labels = np.asarray(mini_batch_labels)\n",
    "            if (i%100==0):\n",
    "                print(\"Current status (every 100 mini-batches)\")\n",
    "                model.fit(mini_batch_in, mini_batch_labels, epochs=1, batch_size=len(this_batch_files), verbose=1)\n",
    "            else:\n",
    "                model.fit(mini_batch_in, mini_batch_labels, epochs=1, batch_size=len(this_batch_files), verbose=0)\n",
    "                \n",
    "    # Validation after a few epochs\n",
    "    test_pred = np.argmax(model.predict(test_in, batch_size=32, verbose=0),axis=1)\n",
    "    confusion_matrix = skm.confusion_matrix(test_labels, test_pred, labels=np.arange(num_accents)) # The last value 5 is number of accents\n",
    "    print(\"Validation Confusion Matrix\")\n",
    "    print(confusion_matrix)\n",
    "    new_metric = skm.accuracy_score(test_labels, test_pred)\n",
    "    print(\"Validation accuracy\")\n",
    "    print(new_metric)\n",
    "\n",
    "    # Define early stopping criteria\n",
    "    if old_metric < new_metric:\n",
    "        old_metric = new_metric\n",
    "        model.save_weights(\"accent.hdf5\")\n",
    "    else:\n",
    "        # Load model\n",
    "        model.load_weights(\"accent.hdf5\")\n",
    "        patience += 1\n",
    "        print(\"Patience: \"+str(patience))\n",
    "    if patience == 3:\n",
    "        print(\"Final confusion matrix\")\n",
    "        print(confusion_matrix)\n",
    "        print(\"Final validation accuracy\")\n",
    "        print(new_metric)\n",
    "        print(\"Done Training\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
